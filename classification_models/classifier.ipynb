{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "class RetinalDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.data_frame.iloc[idx]['id_code'] + '.png')\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        diagnosis = self.data_frame.iloc[idx]['diagnosis']\n",
    "\n",
    "        # Let the feature_extractor handle all the image preprocessing\n",
    "        inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        return inputs['pixel_values'].squeeze(), torch.tensor(diagnosis, dtype=torch.long)\n",
    "\n",
    "class RetinalClassifier:\n",
    "    def __init__(self, num_classes=5):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ViTForImageClassification.from_pretrained(\n",
    "            'google/vit-base-patch16-224',\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def train(self, train_loader, num_epochs=10):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs).logits\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            epoch_loss = running_loss/len(train_loader)\n",
    "            epoch_accuracy = 100 * correct / total\n",
    "            print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    def predict(self, image_path):\n",
    "        self.model.eval()\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image).logits\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "        return predicted.item()\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize dataset and dataloader\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader, random_split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Load full dataset\n",
    "    dataset = RetinalDataset(csv_file='APTOS 2019 Blindness Detection/train.csv', img_dir='APTOS 2019 Blindness Detection//train_images')\n",
    "\n",
    "    # Split dataset (80% train, 20% test)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize and train the classifier\n",
    "    classifier = RetinalClassifier(num_classes=5)\n",
    "    classifier.train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_path = 'best_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': classifier.model.state_dict(),\n",
    "    'model_config': classifier.model.config\n",
    "}, save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from /Users/devshah/Documents/WorkSpace/University/year 3/CSC490/Zero-Shot-Object-Tracking-FPS/classifier_model/Best Model.pth\n",
      "Test Accuracy: 95.36%\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       353\n",
      "           1       0.88      0.92      0.90        73\n",
      "           2       0.93      0.94      0.94       205\n",
      "           3       0.87      0.85      0.86        39\n",
      "           4       0.92      0.87      0.89        63\n",
      "\n",
      "    accuracy                           0.95       733\n",
      "   macro avg       0.92      0.91      0.92       733\n",
      "weighted avg       0.95      0.95      0.95       733\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[351   2   0   0   0]\n",
      " [  1  67   5   0   0]\n",
      " [  0   7 193   4   1]\n",
      " [  0   0   2  33   4]\n",
      " [  0   0   7   1  55]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "classifier = RetinalClassifier(num_classes=5)\n",
    "\n",
    "# Path to your saved model\n",
    "model_path = '/Users/devshah/Documents/WorkSpace/University/year 3/CSC490/Zero-Shot-Object-Tracking-FPS/classifier_model/Best Model.pth'\n",
    "\n",
    "# Load the saved model with map_location to handle CPU/GPU differences\n",
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "classifier.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "# Evaluation loop\n",
    "classifier.model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "predictions = []\n",
    "actual = []\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients during evaluation\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(classifier.device), labels.to(classifier.device)\n",
    "        outputs = classifier.model(inputs).logits\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        actual.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(actual, predictions))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(actual, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
