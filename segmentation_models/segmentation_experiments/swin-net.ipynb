{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "CUDA device name: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "Current CUDA device: 0\n",
      "\n",
      "==================================================\n",
      "Starting retinal segmentation training script using SwinNet architecture\n",
      "==================================================\n",
      "\n",
      "Creating dataset...\n",
      "Found 3662 images in C:/Second_Sem/490/train_images\n",
      "Dataset created with 3662 samples\n",
      "Verifying dataset by loading first sample...\n",
      "Sample image shape: torch.Size([3, 256, 256])\n",
      "Sample mask shape: torch.Size([256, 256])\n",
      "Creating data loader...\n",
      "Initializing segmentation model...\n",
      "Successfully initialized CUDA device\n",
      "Model initialized and moved to cuda\n",
      "Starting training...\n",
      "Training on cuda\n",
      "Training with 916 batches per epoch\n",
      "Starting epoch 1/20\n",
      "  Processing batch 1/916\n",
      "Input shape: torch.Size([4, 3, 256, 256])\n",
      "Output shape: torch.Size([4, 4, 64, 64])\n",
      "Target shape: torch.Size([4, 256, 256])\n",
      "ERROR in training loop (batch 1): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 2): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 3): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 4): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 5): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 6/916\n",
      "ERROR in training loop (batch 6): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 7): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 8): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 9): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 10): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 11/916\n",
      "ERROR in training loop (batch 11): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 12): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 13): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 14): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 15): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 16/916\n",
      "ERROR in training loop (batch 16): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 17): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 18): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 19): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 20): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 21/916\n",
      "ERROR in training loop (batch 21): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 22): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 23): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 24): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 25): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 26/916\n",
      "ERROR in training loop (batch 26): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 27): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 28): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 29): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 30): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 31/916\n",
      "ERROR in training loop (batch 31): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 32): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 33): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 34): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 35): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 36/916\n",
      "ERROR in training loop (batch 36): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 37): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 38): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 39): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 40): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 41/916\n",
      "ERROR in training loop (batch 41): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 42): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 43): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 44): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 45): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 46/916\n",
      "ERROR in training loop (batch 46): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 47): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 48): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 49): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 50): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 51/916\n",
      "ERROR in training loop (batch 51): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 52): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 53): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 54): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 55): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 56/916\n",
      "ERROR in training loop (batch 56): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 57): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 58): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 59): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 60): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 61/916\n",
      "ERROR in training loop (batch 61): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 62): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 63): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 64): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 65): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 66/916\n",
      "ERROR in training loop (batch 66): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 67): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 68): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 69): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 70): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 71/916\n",
      "ERROR in training loop (batch 71): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 72): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 73): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 74): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 75): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 76/916\n",
      "ERROR in training loop (batch 76): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 77): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 78): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 79): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 80): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 81/916\n",
      "ERROR in training loop (batch 81): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 82): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 83): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 84): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 85): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 86/916\n",
      "ERROR in training loop (batch 86): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 87): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 88): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 89): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 90): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 91/916\n",
      "ERROR in training loop (batch 91): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 92): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 93): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 94): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 95): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 96/916\n",
      "ERROR in training loop (batch 96): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 97): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 98): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 99): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 100): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 101/916\n",
      "ERROR in training loop (batch 101): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 102): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 103): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 104): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 105): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 106/916\n",
      "ERROR in training loop (batch 106): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 107): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 108): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 109): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 110): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 111/916\n",
      "ERROR in training loop (batch 111): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 112): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 113): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 114): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 115): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 116/916\n",
      "ERROR in training loop (batch 116): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 117): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 118): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 119): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 120): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 121/916\n",
      "ERROR in training loop (batch 121): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 122): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 123): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 124): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 125): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 126/916\n",
      "ERROR in training loop (batch 126): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 127): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 128): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 129): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 130): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 131/916\n",
      "ERROR in training loop (batch 131): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 132): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 133): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 134): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 135): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 136/916\n",
      "ERROR in training loop (batch 136): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 137): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 138): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 139): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 140): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 141/916\n",
      "ERROR in training loop (batch 141): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 142): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 143): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 144): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 145): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 146/916\n",
      "ERROR in training loop (batch 146): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 147): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 148): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 149): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 150): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "  Processing batch 151/916\n",
      "ERROR in training loop (batch 151): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 152): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 153): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n",
      "ERROR in training loop (batch 154): input and target batch or spatial sizes don't match: target [4, 256, 256], input [4, 4, 64, 64]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 435\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m         \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnhandled exception in main: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 421\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    418\u001b[0m segmentation \u001b[38;5;241m=\u001b[39m RetinalSegmentation(n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 421\u001b[0m \u001b[43msegmentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    424\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(segmentation\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretinal_segmentation_model_swin.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 278\u001b[0m, in \u001b[0;36mRetinalSegmentation.train\u001b[1;34m(self, train_loader, num_epochs)\u001b[0m\n\u001b[0;32m    275\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 278\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Processing batch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_idx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 179\u001b[0m, in \u001b[0;36mRetinalMultiClassDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: Image not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)), torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m--> 179\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# Load masks (assumes filenames match)\u001b[39;00m\n\u001b[0;32m    181\u001b[0m haemorrhage_mask_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhaemorrhages_mask_dir \u001b[38;5;241m/\u001b[39m img_name)\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\PIL\\Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Print CUDA information at the start\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# -------------------------\n",
    "# Swin Transformer Block\n",
    "# -------------------------\n",
    "# A simplified transformer block that mimics a Swin block\n",
    "# (Note: This version uses global attention rather than window-based attention.)\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W) --> reshape to (B, N, C) where N = H*W\n",
    "        B, C, H, W = x.shape\n",
    "        x_flat = x.view(B, C, H * W).permute(0, 2, 1)  # (B, N, C)\n",
    "        x_norm = self.norm1(x_flat)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x_flat = x_flat + attn_out  # residual connection\n",
    "        x_norm2 = self.norm2(x_flat)\n",
    "        mlp_out = self.mlp(x_norm2)\n",
    "        x_flat = x_flat + mlp_out   # residual connection\n",
    "        # Reshape back to (B, C, H, W)\n",
    "        x_out = x_flat.permute(0, 2, 1).view(B, C, H, W)\n",
    "        return x_out\n",
    "\n",
    "# -------------------------\n",
    "# SwinNet Architecture\n",
    "# -------------------------\n",
    "# A U-Net style architecture where the encoder uses a patch embedding\n",
    "# and several SwinTransformerBlocks, with a decoder that upsamples and\n",
    "# uses skip connections.\n",
    "class SwinUnet(nn.Module):\n",
    "    def __init__(self, img_size=256, patch_size=4, in_chans=3, num_classes=4, embed_dim=96):\n",
    "        super(SwinUnet, self).__init__()\n",
    "        # Patch embedding (downsamples by patch_size)\n",
    "        self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        # Encoder stage 1\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            SwinTransformerBlock(embed_dim, num_heads=3),\n",
    "            SwinTransformerBlock(embed_dim, num_heads=3)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(embed_dim, embed_dim * 2, kernel_size=2, stride=2)\n",
    "        # Encoder stage 2\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            SwinTransformerBlock(embed_dim * 2, num_heads=6),\n",
    "            SwinTransformerBlock(embed_dim * 2, num_heads=6)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(embed_dim * 2, embed_dim * 4, kernel_size=2, stride=2)\n",
    "        # Encoder stage 3\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            SwinTransformerBlock(embed_dim * 4, num_heads=12),\n",
    "            SwinTransformerBlock(embed_dim * 4, num_heads=12)\n",
    "        )\n",
    "        self.down3 = nn.Conv2d(embed_dim * 4, embed_dim * 8, kernel_size=2, stride=2)\n",
    "        # Bottleneck (Encoder stage 4)\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            SwinTransformerBlock(embed_dim * 8, num_heads=24),\n",
    "            SwinTransformerBlock(embed_dim * 8, num_heads=24)\n",
    "        )\n",
    "        # Decoder stage 3\n",
    "        self.up3 = nn.ConvTranspose2d(embed_dim * 8, embed_dim * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim * 8, embed_dim * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim * 4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Decoder stage 2\n",
    "        self.up2 = nn.ConvTranspose2d(embed_dim * 4, embed_dim * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim * 4, embed_dim * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim * 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Decoder stage 1\n",
    "        self.up1 = nn.ConvTranspose2d(embed_dim * 2, embed_dim, kernel_size=2, stride=2)\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim * 2, embed_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.out_conv = nn.Conv2d(embed_dim, num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x0 = self.patch_embed(x)  # [B, embed_dim, 256/patch_size, 256/patch_size]\n",
    "        e1 = self.encoder1(x0)      # skip connection 1\n",
    "        x1 = self.down1(e1)         # downsample to [B, embed_dim*2, ...]\n",
    "        e2 = self.encoder2(x1)      # skip connection 2\n",
    "        x2 = self.down2(e2)         # downsample to [B, embed_dim*4, ...]\n",
    "        e3 = self.encoder3(x2)      # skip connection 3\n",
    "        x3 = self.down3(e3)         # downsample to [B, embed_dim*8, ...]\n",
    "        e4 = self.encoder4(x3)      # bottleneck\n",
    "        \n",
    "        # Decoder\n",
    "        d3 = self.up3(e4)           # upsample to match e3 spatial size\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.decoder3(d3)\n",
    "        d2 = self.up2(d3)           # upsample to match e2 spatial size\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.decoder2(d2)\n",
    "        d1 = self.up1(d2)           # upsample to match e1 spatial size\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.decoder1(d1)\n",
    "        out = self.out_conv(d1)\n",
    "        return out\n",
    "\n",
    "# -------------------------\n",
    "# Dataset Definition\n",
    "# -------------------------\n",
    "class RetinalMultiClassDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 image_dir, \n",
    "                 haemorrhages_mask_dir, \n",
    "                 hard_exudates_mask_dir, \n",
    "                 microaneurysm_mask_dir,\n",
    "                 transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.haemorrhages_mask_dir = Path(haemorrhages_mask_dir)\n",
    "        self.hard_exudates_mask_dir = Path(hard_exudates_mask_dir)\n",
    "        self.microaneurysm_mask_dir = Path(microaneurysm_mask_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        for dir_path in [self.image_dir, self.haemorrhages_mask_dir, \n",
    "                         self.hard_exudates_mask_dir, self.microaneurysm_mask_dir]:\n",
    "            if not dir_path.exists():\n",
    "                print(f\"WARNING: Directory does not exist: {dir_path}\")\n",
    "                print(f\"Current working directory: {os.getcwd()}\")\n",
    "                print(f\"Available directories: {os.listdir('.')}\")\n",
    "        \n",
    "        try:\n",
    "            self.images = sorted([f for f in os.listdir(image_dir) \n",
    "                                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            print(f\"Found {len(self.images)} images in {image_dir}\")\n",
    "            if len(self.images) == 0:\n",
    "                print(f\"WARNING: No images found in {image_dir}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Directory not found: {image_dir}\")\n",
    "            self.images = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = self.images[idx]\n",
    "            img_path = str(self.image_dir / img_name)\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"WARNING: Image not found: {img_path}\")\n",
    "                return torch.zeros((3, 256, 256)), torch.zeros((256, 256), dtype=torch.long)\n",
    "            \n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            # Load masks (assumes filenames match)\n",
    "            haemorrhage_mask_path = str(self.haemorrhages_mask_dir / img_name)\n",
    "            hard_exudate_mask_path = str(self.hard_exudates_mask_dir / img_name)\n",
    "            microaneurysm_mask_path = str(self.microaneurysm_mask_dir / img_name)\n",
    "            \n",
    "            if not os.path.exists(haemorrhage_mask_path):\n",
    "                print(f\"WARNING: Haemorrhage mask not found: {haemorrhage_mask_path}\")\n",
    "                haemorrhage_mask = Image.new('L', image.size, 0)\n",
    "            else:\n",
    "                haemorrhage_mask = Image.open(haemorrhage_mask_path).convert('L')\n",
    "            \n",
    "            if not os.path.exists(hard_exudate_mask_path):\n",
    "                print(f\"WARNING: Hard exudate mask not found: {hard_exudate_mask_path}\")\n",
    "                hard_exudate_mask = Image.new('L', image.size, 0)\n",
    "            else:\n",
    "                hard_exudate_mask = Image.open(hard_exudate_mask_path).convert('L')\n",
    "            \n",
    "            if not os.path.exists(microaneurysm_mask_path):\n",
    "                print(f\"WARNING: Microaneurysm mask not found: {microaneurysm_mask_path}\")\n",
    "                microaneurysm_mask = Image.new('L', image.size, 0)\n",
    "            else:\n",
    "                microaneurysm_mask = Image.open(microaneurysm_mask_path).convert('L')\n",
    "            \n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            image = transform(image)\n",
    "            haemorrhage_mask = transform(haemorrhage_mask) > 0.5\n",
    "            hard_exudate_mask = transform(hard_exudate_mask) > 0.5\n",
    "            microaneurysm_mask = transform(microaneurysm_mask) > 0.5\n",
    "            \n",
    "            multi_class_mask = torch.zeros((1, 256, 256), dtype=torch.long)\n",
    "            multi_class_mask[haemorrhage_mask] = 1\n",
    "            multi_class_mask[hard_exudate_mask] = 2\n",
    "            multi_class_mask[microaneurysm_mask] = 3\n",
    "            \n",
    "            return image, multi_class_mask.squeeze(0)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in __getitem__ for index {idx}, image {self.images[idx] if idx < len(self.images) else 'invalid index'}: {str(e)}\")\n",
    "            return torch.zeros((3, 256, 256)), torch.zeros((256, 256), dtype=torch.long)\n",
    "\n",
    "# -------------------------\n",
    "# Segmentation Training Class\n",
    "# -------------------------\n",
    "# Uses CrossEntropyLoss for multi-class segmentation.\n",
    "class RetinalSegmentation:\n",
    "    def __init__(self, n_classes=4):\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                self.device = torch.device('cuda')\n",
    "                test_tensor = torch.zeros(1, device=self.device)\n",
    "                del test_tensor\n",
    "                print(\"Successfully initialized CUDA device\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"CUDA error: {e}\")\n",
    "                print(\"Falling back to CPU\")\n",
    "                self.device = torch.device('cpu')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            try:\n",
    "                self.device = torch.device('mps')\n",
    "                print(\"Using MPS (Metal Performance Shaders) device\")\n",
    "            except Exception:\n",
    "                print(\"MPS initialization failed, falling back to CPU\")\n",
    "                self.device = torch.device('cpu')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            print(\"Using CPU device\")\n",
    "        \n",
    "        try:\n",
    "            # Initialize the SwinUnet model\n",
    "            self.model = SwinUnet(img_size=256, patch_size=4, in_chans=3, num_classes=n_classes, embed_dim=96).to(self.device)\n",
    "            print(f\"Model initialized and moved to {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing model: {e}\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "    def train(self, train_loader, num_epochs=10):\n",
    "        if len(train_loader) == 0:\n",
    "            print(\"ERROR: DataLoader is empty. Cannot train on empty dataset.\")\n",
    "            return\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        \n",
    "        print(f\"Training on {self.device}\")\n",
    "        print(f\"Training with {len(train_loader)} batches per epoch\")\n",
    "        \n",
    "        epoch_losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            start_time = time.time()\n",
    "            print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "                if batch_idx % 5 == 0:\n",
    "                    print(f\"  Processing batch {batch_idx+1}/{len(train_loader)}\")\n",
    "                try:\n",
    "                    images = images.to(self.device)\n",
    "                    masks = masks.to(self.device)\n",
    "                    \n",
    "                    if torch.isnan(images).any() or torch.isnan(masks).any():\n",
    "                        print(f\"WARNING: NaN values detected in input data (batch {batch_idx+1})\")\n",
    "                        continue\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self.model(images)\n",
    "                    if batch_idx == 0 and epoch == 0:\n",
    "                        print(f\"Input shape: {images.shape}\")\n",
    "                        print(f\"Output shape: {outputs.shape}\")\n",
    "                        print(f\"Target shape: {masks.shape}\")\n",
    "                    \n",
    "                    loss = criterion(outputs, masks)\n",
    "                    if torch.isnan(loss):\n",
    "                        print(f\"WARNING: NaN loss detected in batch {batch_idx+1}\")\n",
    "                        continue\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "                    \n",
    "                    if self.device.type == 'cuda':\n",
    "                        torch.cuda.synchronize()\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR in training loop (batch {batch_idx+1}): {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            epoch_losses.append(avg_loss)\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "            \n",
    "            if epoch % 2 == 0:\n",
    "                try:\n",
    "                    self.evaluate_sample(train_loader)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in evaluation: {e}\")\n",
    "        \n",
    "        try:\n",
    "            plt.figure()\n",
    "            plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training Loss Over Epochs')\n",
    "            plt.grid(True)\n",
    "            plt.savefig('training_loss.png')\n",
    "            print(\"Training loss plot saved to 'training_loss.png'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting training loss: {e}\")\n",
    "    \n",
    "    def predict(self, image):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                image = image.to(self.device)\n",
    "                output = self.model(image.unsqueeze(0))\n",
    "                probabilities = F.softmax(output, dim=1)\n",
    "                predicted_mask = torch.argmax(probabilities, dim=1)\n",
    "                return predicted_mask.squeeze().cpu().numpy()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in prediction: {e}\")\n",
    "                return np.zeros((256, 256), dtype=np.int64)\n",
    "    \n",
    "    def evaluate_sample(self, dataloader):\n",
    "        try:\n",
    "            images, masks = next(iter(dataloader))\n",
    "            image = images[0].to(self.device)\n",
    "            mask = masks[0].cpu().numpy()\n",
    "            pred_mask = self.predict(image)\n",
    "            \n",
    "            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            axs[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "            axs[0].set_title('Original Image')\n",
    "            axs[0].axis('off')\n",
    "            colors = ['black', 'red', 'yellow', 'green']\n",
    "            cmap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "            axs[1].imshow(mask, cmap=cmap, vmin=0, vmax=3)\n",
    "            axs[1].set_title('Ground Truth')\n",
    "            axs[1].axis('off')\n",
    "            axs[2].imshow(pred_mask, cmap=cmap, vmin=0, vmax=3)\n",
    "            axs[2].set_title('Prediction')\n",
    "            axs[2].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"sample_evaluation_{time.strftime('%Y%m%d_%H%M%S')}.png\")\n",
    "            print(\"Evaluation sample saved as image file\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in evaluation: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting retinal segmentation training script using SwinNet architecture\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Update these directory paths to match your actual data locations.\n",
    "    image_dir = 'C:/Second_Sem/490/train_images'\n",
    "    haemorrhages_mask_dir = 'C:/Second_Sem/490/APTOS 2019 Blindness Detection Segmented/Haemorrhages/train_images'\n",
    "    hard_exudates_mask_dir = 'C:/Second_Sem/490/APTOS 2019 Blindness Detection Segmented/Hard Exudates/train_images'\n",
    "    microaneurysm_mask_dir = 'C:/Second_Sem/490/APTOS 2019 Blindness Detection Segmented/Microaneurysm/train_images'\n",
    "    \n",
    "    for dir_path in [image_dir, haemorrhages_mask_dir, hard_exudates_mask_dir, microaneurysm_mask_dir]:\n",
    "        if not os.path.exists(dir_path):\n",
    "            print(f\"WARNING: Directory does not exist: {dir_path}\")\n",
    "    \n",
    "    print(\"Creating dataset...\")\n",
    "    dataset = RetinalMultiClassDataset(\n",
    "        image_dir=image_dir,\n",
    "        haemorrhages_mask_dir=haemorrhages_mask_dir,\n",
    "        hard_exudates_mask_dir=hard_exudates_mask_dir,\n",
    "        microaneurysm_mask_dir=microaneurysm_mask_dir\n",
    "    )\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"ERROR: Dataset is empty. Please check your directory paths and image files.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset created with {len(dataset)} samples\")\n",
    "    print(\"Verifying dataset by loading first sample...\")\n",
    "    try:\n",
    "        sample_img, sample_mask = dataset[0]\n",
    "        print(f\"Sample image shape: {sample_img.shape}\")\n",
    "        print(f\"Sample mask shape: {sample_mask.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load sample from dataset: {e}\")\n",
    "    \n",
    "    print(\"Creating data loader...\")\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to >0 for production runs\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(\"Initializing segmentation model...\")\n",
    "    segmentation = RetinalSegmentation(n_classes=4)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    segmentation.train(train_loader, num_epochs=20)\n",
    "    \n",
    "    try:\n",
    "        torch.save(segmentation.model.state_dict(), 'retinal_segmentation_model_swin.pth')\n",
    "        print(\"Model saved successfully to 'retinal_segmentation_model_swin.pth'\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save model: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training completed\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Unhandled exception in main: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
