{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "class ZeroShotTracker:\n",
    "    def __init__(self, clip_model=\"ViT-B/32\"):\n",
    "        \"\"\"\n",
    "        Initialize the zero-shot tracker with CLIP for semantic understanding\n",
    "        and a simple motion predictor.\n",
    "        \"\"\"\n",
    "        # Load CLIP model for zero-shot detection\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.clip_model, self.preprocess = clip.load(clip_model, device=self.device)\n",
    "        \n",
    "        # Initialize tracking state\n",
    "        self.tracking_state = None\n",
    "        self.previous_bbox = None\n",
    "        self.target_embedding = None\n",
    "        \n",
    "        # Configuration\n",
    "        self.search_window = 1.5  # Search window multiplier\n",
    "        self.confidence_threshold = 0.7\n",
    "        \n",
    "    def encode_target(self, target_name):\n",
    "        \"\"\"\n",
    "        Encode the target object name into CLIP embedding space\n",
    "        \"\"\"\n",
    "        text = clip.tokenize([target_name]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.clip_model.encode_text(text)\n",
    "        self.target_embedding = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "    def get_region_proposals(self, frame, prev_bbox):\n",
    "        \"\"\"\n",
    "        Generate region proposals based on previous bbox and motion prediction\n",
    "        \"\"\"\n",
    "        if prev_bbox is None:\n",
    "            # If no previous bbox, search the entire frame\n",
    "            return self._get_sliding_windows(frame.size)\n",
    "            \n",
    "        # Generate proposals around previous bbox\n",
    "        x, y, w, h = prev_bbox\n",
    "        center_x = x + w/2\n",
    "        center_y = y + h/2\n",
    "        \n",
    "        # Create search window\n",
    "        search_w = w * self.search_window\n",
    "        search_h = h * self.search_window\n",
    "        search_x = max(0, center_x - search_w/2)\n",
    "        search_y = max(0, center_y - search_h/2)\n",
    "        \n",
    "        return self._generate_dense_proposals(\n",
    "            (search_x, search_y, search_w, search_h),\n",
    "            frame.size,\n",
    "            num_scales=3\n",
    "        )\n",
    "        \n",
    "    def _generate_dense_proposals(self, search_window, frame_size, num_scales=3):\n",
    "        \"\"\"\n",
    "        Generate dense proposals within search window at multiple scales\n",
    "        \"\"\"\n",
    "        proposals = []\n",
    "        x, y, w, h = search_window\n",
    "        scales = np.linspace(0.8, 1.2, num_scales)\n",
    "        \n",
    "        for scale in scales:\n",
    "            scaled_w = w * scale\n",
    "            scaled_h = h * scale\n",
    "            \n",
    "            # Generate proposals with 50% overlap\n",
    "            step_x = scaled_w * 0.5\n",
    "            step_y = scaled_h * 0.5\n",
    "            \n",
    "            for px in np.arange(x, x + w - scaled_w, step_x):\n",
    "                for py in np.arange(y, y + h - scaled_h, step_y):\n",
    "                    proposals.append((px, py, scaled_w, scaled_h))\n",
    "                    \n",
    "        return proposals\n",
    "        \n",
    "    def track(self, frame, target_name=None):\n",
    "        \"\"\"\n",
    "        Track the target object in the given frame\n",
    "        \"\"\"\n",
    "        if target_name is not None:\n",
    "            self.encode_target(target_name)\n",
    "            \n",
    "        if self.target_embedding is None:\n",
    "            raise ValueError(\"No target object specified\")\n",
    "            \n",
    "        # Convert frame to PIL if needed\n",
    "        if not isinstance(frame, Image.Image):\n",
    "            frame = Image.fromarray(frame)\n",
    "            \n",
    "        # Get region proposals\n",
    "        proposals = self.get_region_proposals(frame, self.previous_bbox)\n",
    "        \n",
    "        # Score each proposal using CLIP\n",
    "        best_score = -float('inf')\n",
    "        best_bbox = None\n",
    "        \n",
    "        for bbox in proposals:\n",
    "            x, y, w, h = [int(v) for v in bbox]\n",
    "            region = frame.crop((x, y, x+w, y+h))\n",
    "            score = self._compute_clip_score(region)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_bbox = bbox\n",
    "                \n",
    "        # Update tracking state\n",
    "        if best_score > self.confidence_threshold:\n",
    "            self.previous_bbox = best_bbox\n",
    "            return best_bbox, best_score\n",
    "        else:\n",
    "            self.previous_bbox = None\n",
    "            return None, best_score\n",
    "            \n",
    "    def _compute_clip_score(self, region):\n",
    "        \"\"\"\n",
    "        Compute CLIP similarity score between image region and target text\n",
    "        \"\"\"\n",
    "        # Preprocess image\n",
    "        image = self.preprocess(region).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Get image features\n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.encode_image(image)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "        # Compute similarity\n",
    "        similarity = F.cosine_similarity(image_features, self.target_embedding)\n",
    "        return similarity.item()\n",
    "        \n",
    "    def _get_sliding_windows(self, frame_size, min_size=64):\n",
    "        \"\"\"\n",
    "        Generate sliding window proposals for initial detection\n",
    "        \"\"\"\n",
    "        width, height = frame_size\n",
    "        proposals = []\n",
    "        \n",
    "        # Generate windows at multiple scales\n",
    "        for scale in np.linspace(0.1, 0.5, 5):\n",
    "            w = int(width * scale)\n",
    "            h = int(height * scale)\n",
    "            \n",
    "            if w < min_size or h < min_size:\n",
    "                continue\n",
    "                \n",
    "            step_x = w // 2\n",
    "            step_y = h // 2\n",
    "            \n",
    "            for x in range(0, width - w, step_x):\n",
    "                for y in range(0, height - h, step_y):\n",
    "                    proposals.append((x, y, w, h))\n",
    "                    \n",
    "        return proposals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
