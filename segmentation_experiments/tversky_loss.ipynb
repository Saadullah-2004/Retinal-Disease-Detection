{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "CUDA device name: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "Current CUDA device: 0\n",
      "\n",
      "==================================================\n",
      "Starting retinal segmentation training script\n",
      "==================================================\n",
      "\n",
      "Creating dataset...\n",
      "Found 3662 images in C:/Second_Sem/490/train_images\n",
      "Dataset created with 3662 samples\n",
      "Verifying dataset by loading first sample...\n",
      "Sample image shape: torch.Size([3, 256, 256])\n",
      "Sample mask shape: torch.Size([256, 256])\n",
      "Creating data loader...\n",
      "Initializing segmentation model...\n",
      "Successfully initialized CUDA device\n",
      "Model initialized and moved to cuda\n",
      "Starting training...\n",
      "Training on cuda\n",
      "Training with 916 batches per epoch\n",
      "Starting epoch 1/20\n",
      "  Processing batch 1/916\n",
      "Input shape: torch.Size([4, 3, 256, 256])\n",
      "Output shape: torch.Size([4, 4, 256, 256])\n",
      "Target shape: torch.Size([4, 256, 256])\n",
      "  Processing batch 6/916\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 447\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 447\u001b[0m         \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    449\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnhandled exception in main: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 433\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    430\u001b[0m segmentation \u001b[38;5;241m=\u001b[39m RetinalSegmentation(n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 433\u001b[0m \u001b[43msegmentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(segmentation\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretinal_segmentation_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 279\u001b[0m, in \u001b[0;36mRetinalSegmentation.train\u001b[1;34m(self, train_loader, num_epochs)\u001b[0m\n\u001b[0;32m    275\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 279\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  Processing batch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_idx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 208\u001b[0m, in \u001b[0;36mRetinalMultiClassDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    205\u001b[0m image \u001b[38;5;241m=\u001b[39m transform(image)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# Transform masks and convert to binary (0 or 1)\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m haemorrhage_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhaemorrhage_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m    209\u001b[0m hard_exudate_mask \u001b[38;5;241m=\u001b[39m transform(hard_exudate_mask) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m    210\u001b[0m microaneurysm_mask \u001b[38;5;241m=\u001b[39m transform(microaneurysm_mask) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Second_Sem\\490\\env\\Lib\\site-packages\\PIL\\Image.py:2365\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2353\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2354\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2355\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m   2356\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2357\u001b[0m         )\n\u001b[0;32m   2358\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2359\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2360\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2361\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2362\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2363\u001b[0m         )\n\u001b[1;32m-> 2365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Print CUDA information at the start\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# Define the Tversky loss function\n",
    "def tversky_loss(y_pred, y_true, alpha=0.5, beta=0.5, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the Tversky loss for multi-class segmentation.\n",
    "    y_pred: [B, n_classes, H, W] (logits, will be softmaxed inside)\n",
    "    y_true: [B, H, W] with class indices\n",
    "    \"\"\"\n",
    "    n_classes = y_pred.shape[1]\n",
    "    # Convert target to one-hot encoding\n",
    "    y_true_one_hot = F.one_hot(y_true, num_classes=n_classes).permute(0, 3, 1, 2).float()\n",
    "    # Apply softmax to get probabilities\n",
    "    y_pred = F.softmax(y_pred, dim=1)\n",
    "    \n",
    "    # Compute true positives, false positives and false negatives per class\n",
    "    dims = (0, 2, 3)\n",
    "    TP = (y_pred * y_true_one_hot).sum(dims)\n",
    "    FP = (y_pred * (1 - y_true_one_hot)).sum(dims)\n",
    "    FN = ((1 - y_pred) * y_true_one_hot).sum(dims)\n",
    "    \n",
    "    tversky = (TP + smooth) / (TP + alpha * FN + beta * FP + smooth)\n",
    "    loss = 1 - tversky\n",
    "    return loss.mean()\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=4):  # 4 classes: background, haemorrhages, hard exudates, microaneurysm\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(64, 128)\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(128, 256)\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(256, 512)\n",
    "        )\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(512, 1024)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.up_conv1 = DoubleConv(1024, 512)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up_conv2 = DoubleConv(512, 256)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.up_conv3 = DoubleConv(256, 128)\n",
    "        \n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.up_conv4 = DoubleConv(128, 64)\n",
    "        \n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up1(x5)\n",
    "        x = torch.cat([x4, x], dim=1)\n",
    "        x = self.up_conv1(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x3, x], dim=1)\n",
    "        x = self.up_conv2(x)\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x2, x], dim=1)\n",
    "        x = self.up_conv3(x)\n",
    "        \n",
    "        x = self.up4(x)\n",
    "        x = torch.cat([x1, x], dim=1)\n",
    "        x = self.up_conv4(x)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "class RetinalMultiClassDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 image_dir, \n",
    "                 haemorrhages_mask_dir, \n",
    "                 hard_exudates_mask_dir, \n",
    "                 microaneurysm_mask_dir,\n",
    "                 transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.haemorrhages_mask_dir = Path(haemorrhages_mask_dir)\n",
    "        self.hard_exudates_mask_dir = Path(hard_exudates_mask_dir)\n",
    "        self.microaneurysm_mask_dir = Path(microaneurysm_mask_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Verify directories exist\n",
    "        for dir_path in [self.image_dir, self.haemorrhages_mask_dir, self.hard_exudates_mask_dir, self.microaneurysm_mask_dir]:\n",
    "            if not dir_path.exists():\n",
    "                print(f\"WARNING: Directory does not exist: {dir_path}\")\n",
    "                print(f\"Current working directory: {os.getcwd()}\")\n",
    "                print(f\"Available directories: {os.listdir('.')}\")\n",
    "\n",
    "        # Get list of image files\n",
    "        try:\n",
    "            self.images = sorted([f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            print(f\"Found {len(self.images)} images in {image_dir}\")\n",
    "            if len(self.images) == 0:\n",
    "                print(f\"WARNING: No images found in {image_dir}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Directory not found: {image_dir}\")\n",
    "            self.images = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = self.images[idx]\n",
    "            img_path = str(self.image_dir / img_name)\n",
    "            \n",
    "            # Check if image exists\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"WARNING: Image not found: {img_path}\")\n",
    "                # Return a dummy tensor if image not found\n",
    "                return torch.zeros((3, 256, 256)), torch.zeros((256, 256), dtype=torch.long)\n",
    "            \n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Load individual masks; assuming mask filenames match image filenames\n",
    "            haemorrhage_mask_path = str(self.haemorrhages_mask_dir / img_name)\n",
    "            hard_exudate_mask_path = str(self.hard_exudates_mask_dir / img_name)\n",
    "            microaneurysm_mask_path = str(self.microaneurysm_mask_dir / img_name)\n",
    "            \n",
    "            # Check if mask files exist\n",
    "            if not os.path.exists(haemorrhage_mask_path):\n",
    "                print(f\"WARNING: Haemorrhage mask not found: {haemorrhage_mask_path}\")\n",
    "                haemorrhage_mask = Image.new('L', image.size, 0)\n",
    "            else:\n",
    "                haemorrhage_mask = Image.open(haemorrhage_mask_path).convert('L')\n",
    "            \n",
    "            if not os.path.exists(hard_exudate_mask_path):\n",
    "                print(f\"WARNING: Hard exudate mask not found: {hard_exudate_mask_path}\")\n",
    "                hard_exudate_mask = Image.new('L', image.size, 0)\n",
    "            else:\n",
    "                hard_exudate_mask = Image.open(hard_exudate_mask_path).convert('L')\n",
    "            \n",
    "            if not os.path.exists(microaneurysm_mask_path):\n",
    "                print(f\"WARNING: Microaneurysm mask not found: {microaneurysm_mask_path}\")\n",
    "                microaneurysm_mask = Image.new('L', image.size, 0)\n",
    "            else:\n",
    "                microaneurysm_mask = Image.open(microaneurysm_mask_path).convert('L')\n",
    "            \n",
    "            # Apply transforms to image and masks\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            \n",
    "            image = transform(image)\n",
    "            \n",
    "            # Transform masks and convert to binary (0 or 1)\n",
    "            haemorrhage_mask = transform(haemorrhage_mask) > 0.5\n",
    "            hard_exudate_mask = transform(hard_exudate_mask) > 0.5\n",
    "            microaneurysm_mask = transform(microaneurysm_mask) > 0.5\n",
    "            \n",
    "            # Create a multi-class mask where:\n",
    "            # 0: Background, 1: Haemorrhages, 2: Hard Exudates, 3: Microaneurysms\n",
    "            multi_class_mask = torch.zeros((1, 256, 256), dtype=torch.long)\n",
    "            multi_class_mask[haemorrhage_mask] = 1\n",
    "            multi_class_mask[hard_exudate_mask] = 2\n",
    "            multi_class_mask[microaneurysm_mask] = 3\n",
    "            \n",
    "            return image, multi_class_mask.squeeze(0)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in __getitem__ for index {idx}, image {self.images[idx] if idx < len(self.images) else 'invalid index'}: {str(e)}\")\n",
    "            # Return dummy tensors on error\n",
    "            return torch.zeros((3, 256, 256)), torch.zeros((256, 256), dtype=torch.long)\n",
    "\n",
    "class RetinalSegmentation:\n",
    "    def __init__(self, n_classes=4):\n",
    "        # Set device with explicit error handling\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                self.device = torch.device('cuda')\n",
    "                test_tensor = torch.zeros(1, device=self.device)\n",
    "                del test_tensor\n",
    "                print(\"Successfully initialized CUDA device\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"CUDA error: {e}\")\n",
    "                print(\"Falling back to CPU\")\n",
    "                self.device = torch.device('cpu')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            try:\n",
    "                self.device = torch.device('mps')\n",
    "                print(\"Using MPS (Metal Performance Shaders) device\")\n",
    "            except Exception:\n",
    "                print(\"MPS initialization failed, falling back to CPU\")\n",
    "                self.device = torch.device('cpu')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            print(\"Using CPU device\")\n",
    "        \n",
    "        try:\n",
    "            self.model = UNet(n_channels=3, n_classes=n_classes).to(self.device)\n",
    "            print(f\"Model initialized and moved to {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing model: {e}\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "    def train(self, train_loader, num_epochs=10):\n",
    "        if len(train_loader) == 0:\n",
    "            print(\"ERROR: DataLoader is empty. Cannot train on empty dataset.\")\n",
    "            return\n",
    "\n",
    "        # Use the custom Tversky loss for multi-class segmentation\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        \n",
    "        print(f\"Training on {self.device}\")\n",
    "        print(f\"Training with {len(train_loader)} batches per epoch\")\n",
    "        \n",
    "        epoch_losses = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "                if batch_idx % 5 == 0:\n",
    "                    print(f\"  Processing batch {batch_idx+1}/{len(train_loader)}\")\n",
    "                \n",
    "                try:\n",
    "                    images = images.to(self.device)\n",
    "                    masks = masks.to(self.device)\n",
    "                    \n",
    "                    if torch.isnan(images).any() or torch.isnan(masks).any():\n",
    "                        print(f\"WARNING: NaN values detected in input data (batch {batch_idx+1})\")\n",
    "                        continue\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self.model(images)  # [B, C, H, W]\n",
    "                    \n",
    "                    if batch_idx == 0 and epoch == 0:\n",
    "                        print(f\"Input shape: {images.shape}\")\n",
    "                        print(f\"Output shape: {outputs.shape}\")\n",
    "                        print(f\"Target shape: {masks.shape}\")\n",
    "                    \n",
    "                    loss = tversky_loss(outputs, masks)\n",
    "                    \n",
    "                    if torch.isnan(loss):\n",
    "                        print(f\"WARNING: NaN loss detected in batch {batch_idx+1}\")\n",
    "                        continue\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item()\n",
    "                    \n",
    "                    if self.device.type == 'cuda':\n",
    "                        torch.cuda.synchronize()\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR in training loop (batch {batch_idx+1}): {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            epoch_losses.append(avg_loss)\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "            \n",
    "            if epoch % 2 == 0:\n",
    "                try:\n",
    "                    self.evaluate_sample(train_loader)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in evaluation: {e}\")\n",
    "        \n",
    "        try:\n",
    "            plt.figure()\n",
    "            plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training Loss Over Epochs')\n",
    "            plt.grid(True)\n",
    "            plt.savefig('training_loss.png')\n",
    "            print(\"Training loss plot saved to 'training_loss.png'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting training loss: {e}\")\n",
    "    \n",
    "    def predict(self, image):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                image = image.to(self.device)\n",
    "                output = self.model(image.unsqueeze(0))\n",
    "                probabilities = F.softmax(output, dim=1)\n",
    "                predicted_mask = torch.argmax(probabilities, dim=1)\n",
    "                return predicted_mask.squeeze().cpu().numpy()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in prediction: {e}\")\n",
    "                return np.zeros((256, 256), dtype=np.int64)\n",
    "    \n",
    "    def evaluate_sample(self, dataloader):\n",
    "        try:\n",
    "            images, masks = next(iter(dataloader))\n",
    "            image = images[0].to(self.device)\n",
    "            mask = masks[0].cpu().numpy()\n",
    "            \n",
    "            pred_mask = self.predict(image)\n",
    "            \n",
    "            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            axs[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "            axs[0].set_title('Original Image')\n",
    "            axs[0].axis('off')\n",
    "            \n",
    "            colors = ['black', 'red', 'yellow', 'green']\n",
    "            cmap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "            axs[1].imshow(mask, cmap=cmap, vmin=0, vmax=3)\n",
    "            axs[1].set_title('Ground Truth')\n",
    "            axs[1].axis('off')\n",
    "            \n",
    "            axs[2].imshow(pred_mask, cmap=cmap, vmin=0, vmax=3)\n",
    "            axs[2].set_title('Prediction')\n",
    "            axs[2].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"sample_evaluation_{time.strftime('%Y%m%d_%H%M%S')}.png\")\n",
    "            print(\"Evaluation sample saved as image file\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in evaluation: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting retinal segmentation training script\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    image_dir = 'C:/Second_Sem/490/train_images'\n",
    "    haemorrhages_mask_dir = 'C:/Second_Sem/490/APTOS 2019 Blindness Detection Segmented/Haemorrhages/train_images'\n",
    "    hard_exudates_mask_dir = 'C:/Second_Sem/490/APTOS 2019 Blindness Detection Segmented/Hard Exudates/train_images'\n",
    "    microaneurysm_mask_dir = 'C:/Second_Sem/490/APTOS 2019 Blindness Detection Segmented/Microaneurysm/train_images'\n",
    "    \n",
    "    for dir_path in [image_dir, haemorrhages_mask_dir, hard_exudates_mask_dir, microaneurysm_mask_dir]:\n",
    "        if not os.path.exists(dir_path):\n",
    "            print(f\"WARNING: Directory does not exist: {dir_path}\")\n",
    "    \n",
    "    print(\"Creating dataset...\")\n",
    "    dataset = RetinalMultiClassDataset(\n",
    "        image_dir=image_dir,\n",
    "        haemorrhages_mask_dir=haemorrhages_mask_dir,\n",
    "        hard_exudates_mask_dir=hard_exudates_mask_dir,\n",
    "        microaneurysm_mask_dir=microaneurysm_mask_dir\n",
    "    )\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"ERROR: Dataset is empty. Please check your directory paths and image files.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset created with {len(dataset)} samples\")\n",
    "    \n",
    "    print(\"Verifying dataset by loading first sample...\")\n",
    "    try:\n",
    "        sample_img, sample_mask = dataset[0]\n",
    "        print(f\"Sample image shape: {sample_img.shape}\")\n",
    "        print(f\"Sample mask shape: {sample_mask.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load sample from dataset: {e}\")\n",
    "    \n",
    "    print(\"Creating data loader...\")\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to >0 for production runs\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(\"Initializing segmentation model...\")\n",
    "    segmentation = RetinalSegmentation(n_classes=4)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    segmentation.train(train_loader, num_epochs=20)\n",
    "    \n",
    "    try:\n",
    "        torch.save(segmentation.model.state_dict(), 'retinal_segmentation_model.pth')\n",
    "        print(\"Model saved successfully to 'retinal_segmentation_model.pth'\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save model: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training completed\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Unhandled exception in main: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
