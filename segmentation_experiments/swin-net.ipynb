{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "CUDA device name: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "Current CUDA device: 0\n",
      "\n",
      "==================================================\n",
      "Starting retinal segmentation training script using SwinNet architecture\n",
      "==================================================\n",
      "\n",
      "Creating dataset...\n",
      "Found 3662 images in C:/Second_Sem/490/train_images\n",
      "Dataset created with 3662 samples\n",
      "Verifying dataset by loading first sample...\n",
      "Sample image shape: torch.Size([3, 256, 256])\n",
      "Sample mask shape: torch.Size([256, 256])\n",
      "Creating data loader...\n",
      "Initializing segmentation model...\n",
      "Successfully initialized CUDA device\n",
      "Model initialized and moved to cuda\n",
      "Starting training...\n",
      "Training on cuda\n",
      "Training with 916 batches per epoch\n",
      "Starting epoch 1/20\n",
      "  Processing batch 1/916\n",
      "Input shape:  torch.Size([4, 3, 256, 256])\n",
      "Output shape: torch.Size([4, 4, 256, 256])\n",
      "Target shape: torch.Size([4, 256, 256])\n",
      "  Processing batch 6/916\n",
      "  Processing batch 11/916\n",
      "  Processing batch 16/916\n",
      "  Processing batch 21/916\n",
      "  Processing batch 26/916\n",
      "  Processing batch 31/916\n",
      "  Processing batch 36/916\n",
      "  Processing batch 41/916\n",
      "  Processing batch 46/916\n",
      "  Processing batch 51/916\n",
      "  Processing batch 56/916\n",
      "  Processing batch 61/916\n",
      "  Processing batch 66/916\n",
      "  Processing batch 71/916\n",
      "  Processing batch 76/916\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 494\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 494\u001b[0m         \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnhandled exception in main: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 480\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    477\u001b[0m segmentation \u001b[38;5;241m=\u001b[39m RetinalSegmentation(n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 480\u001b[0m \u001b[43msegmentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(segmentation\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretinal_segmentation_model_swin.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 352\u001b[0m, in \u001b[0;36mRetinalSegmentation.train\u001b[1;34m(self, train_loader, num_epochs)\u001b[0m\n\u001b[0;32m    350\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    351\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 352\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    355\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Print CUDA information at the start\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# 1) Simplified Swin Transformer Block (using global attention, not window-based)\n",
    "# ------------------------------------------------------------------------------------\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (B, C, H, W)\n",
    "        We'll flatten the spatial dimensions to (B, N, C) where N = H*W\n",
    "        Then apply LayerNorm + MHA + MLP with skip connections.\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        # Flatten (B, C, H, W) -> (B, N, C), N=H*W\n",
    "        x_flat = x.view(B, C, H * W).permute(0, 2, 1)  # (B, N, C)\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        x_norm = self.norm1(x_flat)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)  # global attention\n",
    "        x_flat = x_flat + attn_out  # residual\n",
    "        \n",
    "        # MLP\n",
    "        x_norm2 = self.norm2(x_flat)\n",
    "        mlp_out = self.mlp(x_norm2)\n",
    "        x_flat = x_flat + mlp_out  # residual\n",
    "        \n",
    "        # Reshape back to (B, C, H, W)\n",
    "        x_out = x_flat.permute(0, 2, 1).view(B, C, H, W)\n",
    "        return x_out\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# 2) SwinUnet Architecture with final upsample to 256×256\n",
    "# ------------------------------------------------------------------------------------\n",
    "class SwinUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A U-Net–like encoder/decoder that uses simplified SwinTransformerBlock modules\n",
    "    in each stage. Because of patch_size=4 and multiple downsampling stages,\n",
    "    the feature maps shrink to 1/4, then 1/8, 1/16, 1/32, etc. We ensure we \n",
    "    upsample back to 256×256 at the end to match the ground truth mask size.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=256, patch_size=4, in_chans=3, num_classes=4, embed_dim=96):\n",
    "        super(SwinUnet, self).__init__()\n",
    "        \n",
    "        # (A) Patch embedding (downsamples by patch_size=4)\n",
    "        self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # (B) Encoder stage 1\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            SwinTransformerBlock(embed_dim, num_heads=3),\n",
    "            SwinTransformerBlock(embed_dim, num_heads=3)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(embed_dim, embed_dim * 2, kernel_size=2, stride=2)  # /2\n",
    "        \n",
    "        # (C) Encoder stage 2\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            SwinTransformerBlock(embed_dim * 2, num_heads=6),\n",
    "            SwinTransformerBlock(embed_dim * 2, num_heads=6)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(embed_dim * 2, embed_dim * 4, kernel_size=2, stride=2)  # /2\n",
    "        \n",
    "        # (D) Encoder stage 3\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            SwinTransformerBlock(embed_dim * 4, num_heads=12),\n",
    "            SwinTransformerBlock(embed_dim * 4, num_heads=12)\n",
    "        )\n",
    "        self.down3 = nn.Conv2d(embed_dim * 4, embed_dim * 8, kernel_size=2, stride=2)  # /2\n",
    "        \n",
    "        # (E) Bottleneck (Encoder stage 4)\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            SwinTransformerBlock(embed_dim * 8, num_heads=24),\n",
    "            SwinTransformerBlock(embed_dim * 8, num_heads=24)\n",
    "        )\n",
    "        \n",
    "        # (F) Decoder stage 3\n",
    "        self.up3 = nn.ConvTranspose2d(embed_dim * 8, embed_dim * 4, kernel_size=2, stride=2)  # x2\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim * 8, embed_dim * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim * 4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # (G) Decoder stage 2\n",
    "        self.up2 = nn.ConvTranspose2d(embed_dim * 4, embed_dim * 2, kernel_size=2, stride=2)  # x2\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim * 4, embed_dim * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim * 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # (H) Decoder stage 1\n",
    "        self.up1 = nn.ConvTranspose2d(embed_dim * 2, embed_dim, kernel_size=2, stride=2)  # x2\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim * 2, embed_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # (I) Final upsample to get back from 64×64 to 256×256\n",
    "        # Because we've done 1 patch_embed (stride=4) and 3 downsamplings (each stride=2),\n",
    "        # overall we have a factor of 4 * 2 * 2 * 2 = 32 in the encoder. The decoder so far\n",
    "        # upsamples by 2×2×2 = 8. So we end up at 1/4 of the original (64×64).\n",
    "        # We need a final factor-of-4 upsample to restore 256×256.\n",
    "        self.final_up = nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=4, stride=4)\n",
    "        \n",
    "        # (J) Output convolution\n",
    "        self.out_conv = nn.Conv2d(embed_dim, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1) Patch embedding\n",
    "        x0 = self.patch_embed(x)     # B, embed_dim, 256/4=64, 256/4=64\n",
    "        \n",
    "        # 2) Encoder\n",
    "        e1 = self.encoder1(x0)       # skip 1\n",
    "        x1 = self.down1(e1)          # B, embed_dim*2, 32, 32\n",
    "        e2 = self.encoder2(x1)       # skip 2\n",
    "        x2 = self.down2(e2)          # B, embed_dim*4, 16, 16\n",
    "        e3 = self.encoder3(x2)       # skip 3\n",
    "        x3 = self.down3(e3)          # B, embed_dim*8, 8, 8\n",
    "        e4 = self.encoder4(x3)       # bottleneck, B, embed_dim*8, 8, 8\n",
    "        \n",
    "        # 3) Decoder\n",
    "        d3 = self.up3(e4)            # B, embed_dim*4, 16, 16\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.decoder3(d3)       # B, embed_dim*4, 16, 16\n",
    "        \n",
    "        d2 = self.up2(d3)            # B, embed_dim*2, 32, 32\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.decoder2(d2)       # B, embed_dim*2, 32, 32\n",
    "        \n",
    "        d1 = self.up1(d2)            # B, embed_dim, 64, 64\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.decoder1(d1)       # B, embed_dim, 64, 64\n",
    "        \n",
    "        # 4) Final upsample from 64×64 to 256×256\n",
    "        out_up = self.final_up(d1)   # B, embed_dim, 256, 256\n",
    "        \n",
    "        # 5) Final conv\n",
    "        out = self.out_conv(out_up)  # B, num_classes, 256, 256\n",
    "        return out\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# 3) RetinalMultiClassDataset\n",
    "# ------------------------------------------------------------------------------------\n",
    "class RetinalMultiClassDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 image_dir, \n",
    "                 haemorrhages_mask_dir, \n",
    "                 hard_exudates_mask_dir, \n",
    "                 microaneurysm_mask_dir,\n",
    "                 transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.haemorrhages_mask_dir = Path(haemorrhages_mask_dir)\n",
    "        self.hard_exudates_mask_dir = Path(hard_exudates_mask_dir)\n",
    "        self.microaneurysm_mask_dir = Path(microaneurysm_mask_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        for dir_path in [self.image_dir, self.haemorrhages_mask_dir, \n",
    "                         self.hard_exudates_mask_dir, self.microaneurysm_mask_dir]:\n",
    "            if not dir_path.exists():\n",
    "                print(f\"WARNING: Directory does not exist: {dir_path}\")\n",
    "                print(f\"Current working directory: {os.getcwd()}\")\n",
    "                print(f\"Available directories: {os.listdir('.')}\")\n",
    "        \n",
    "        try:\n",
    "            self.images = sorted([f for f in os.listdir(image_dir) \n",
    "                                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            print(f\"Found {len(self.images)} images in {image_dir}\")\n",
    "            if len(self.images) == 0:\n",
    "                print(f\"WARNING: No images found in {image_dir}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Directory not found: {image_dir}\")\n",
    "            self.images = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = self.images[idx]\n",
    "            img_path = str(self.image_dir / img_name)\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"WARNING: Image not found: {img_path}\")\n",
    "                return torch.zeros((3, 256, 256)), torch.zeros((256, 256), dtype=torch.long)\n",
    "            \n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            # Load masks (assumes filenames match)\n",
    "            haemorrhage_mask_path = str(self.haemorrhages_mask_dir / img_name)\n",
    "            hard_exudate_mask_path = str(self.hard_exudates_mask_dir / img_name)\n",
    "            microaneurysm_mask_path = str(self.microaneurysm_mask_dir / img_name)\n",
    "            \n",
    "            if not os.path.exists(haemorrhage_mask_path):\n",
    "                print(f\"WARNING: Haemorrhage mask not found: {haemorrhage_mask_path}\")\n",
    "                haemorrhage_mask = Image.new('L', image.size, 0)\n",
    "            else:\n",
    "                haemorrhage_mask = Image.open(haemorrhage_mask_path).convert('L')\n",
    "            \n",
    "            if not os.path.exists(hard_exudate_mask_path):\n",
    "                print(f\"WARNING: Hard exudate mask not found: {hard_exudate_mask_path}\")\n",
    "                hard_exudate_mask = Image.new('L', image.size, 0)\n",
    "            else:\n",
    "                hard_exudate_mask = Image.open(hard_exudate_mask_path).convert('L')\n",
    "            \n",
    "            if not os.path.exists(microaneurysm_mask_path):\n",
    "                print(f\"WARNING: Microaneurysm mask not found: {microaneurysm_mask_path}\")\n",
    "                microaneurysm_mask = Image.new('L', image.size, 0)\n",
    "            else:\n",
    "                microaneurysm_mask = Image.open(microaneurysm_mask_path).convert('L')\n",
    "            \n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            image = transform(image)\n",
    "            \n",
    "            # Convert each mask to boolean then create multi-class\n",
    "            haemorrhage_mask = transform(haemorrhage_mask) > 0.5\n",
    "            hard_exudate_mask = transform(hard_exudate_mask) > 0.5\n",
    "            microaneurysm_mask = transform(microaneurysm_mask) > 0.5\n",
    "            \n",
    "            multi_class_mask = torch.zeros((1, 256, 256), dtype=torch.long)\n",
    "            multi_class_mask[haemorrhage_mask] = 1\n",
    "            multi_class_mask[hard_exudate_mask] = 2\n",
    "            multi_class_mask[microaneurysm_mask] = 3\n",
    "            \n",
    "            return image, multi_class_mask.squeeze(0)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in __getitem__ for index {idx}, image {self.images[idx] if idx < len(self.images) else 'invalid index'}: {str(e)}\")\n",
    "            return torch.zeros((3, 256, 256)), torch.zeros((256, 256), dtype=torch.long)\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# 4) Training / Segmentation Class (using CrossEntropyLoss)\n",
    "# ------------------------------------------------------------------------------------\n",
    "class RetinalSegmentation:\n",
    "    def __init__(self, n_classes=4):\n",
    "        # Device setup\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                self.device = torch.device('cuda')\n",
    "                test_tensor = torch.zeros(1, device=self.device)\n",
    "                del test_tensor\n",
    "                print(\"Successfully initialized CUDA device\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"CUDA error: {e}\")\n",
    "                print(\"Falling back to CPU\")\n",
    "                self.device = torch.device('cpu')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            try:\n",
    "                self.device = torch.device('mps')\n",
    "                print(\"Using MPS (Metal Performance Shaders) device\")\n",
    "            except Exception:\n",
    "                print(\"MPS initialization failed, falling back to CPU\")\n",
    "                self.device = torch.device('cpu')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            print(\"Using CPU device\")\n",
    "        \n",
    "        # Initialize the SwinUnet model\n",
    "        try:\n",
    "            self.model = SwinUnet(\n",
    "                img_size=256,   # input image size\n",
    "                patch_size=4,   # patch embedding stride\n",
    "                in_chans=3,\n",
    "                num_classes=n_classes,\n",
    "                embed_dim=96    # base embedding dimension\n",
    "            ).to(self.device)\n",
    "            print(f\"Model initialized and moved to {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing model: {e}\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "    def train(self, train_loader, num_epochs=10):\n",
    "        if len(train_loader) == 0:\n",
    "            print(\"ERROR: DataLoader is empty. Cannot train on empty dataset.\")\n",
    "            return\n",
    "\n",
    "        # CrossEntropyLoss for multi-class segmentation\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        \n",
    "        print(f\"Training on {self.device}\")\n",
    "        print(f\"Training with {len(train_loader)} batches per epoch\")\n",
    "        \n",
    "        epoch_losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            start_time = time.time()\n",
    "            print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "                # Just to see some progress\n",
    "                if batch_idx % 5 == 0:\n",
    "                    print(f\"  Processing batch {batch_idx+1}/{len(train_loader)}\")\n",
    "                \n",
    "                try:\n",
    "                    images = images.to(self.device)\n",
    "                    masks = masks.to(self.device)  # shape [B, 256, 256]\n",
    "                    \n",
    "                    if torch.isnan(images).any() or torch.isnan(masks).any():\n",
    "                        print(f\"WARNING: NaN values detected in input data (batch {batch_idx+1})\")\n",
    "                        continue\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self.model(images)   # shape [B, n_classes, 256, 256]\n",
    "                    \n",
    "                    if batch_idx == 0 and epoch == 0:\n",
    "                        print(f\"Input shape:  {images.shape}\")\n",
    "                        print(f\"Output shape: {outputs.shape}\")\n",
    "                        print(f\"Target shape: {masks.shape}\")\n",
    "                    \n",
    "                    loss = criterion(outputs, masks)\n",
    "                    if torch.isnan(loss):\n",
    "                        print(f\"WARNING: NaN loss detected in batch {batch_idx+1}\")\n",
    "                        continue\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "                    \n",
    "                    if self.device.type == 'cuda':\n",
    "                        torch.cuda.synchronize()\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR in training loop (batch {batch_idx+1}): {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            epoch_losses.append(avg_loss)\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "            \n",
    "            # Optionally evaluate a sample\n",
    "            if epoch % 2 == 0:\n",
    "                try:\n",
    "                    self.evaluate_sample(train_loader)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in evaluation: {e}\")\n",
    "        \n",
    "        # Plot the training loss\n",
    "        try:\n",
    "            plt.figure()\n",
    "            plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training Loss Over Epochs')\n",
    "            plt.grid(True)\n",
    "            plt.savefig('training_loss.png')\n",
    "            print(\"Training loss plot saved to 'training_loss.png'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting training loss: {e}\")\n",
    "    \n",
    "    def predict(self, image):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                image = image.to(self.device)\n",
    "                output = self.model(image.unsqueeze(0))  # [1, n_classes, 256, 256]\n",
    "                probabilities = F.softmax(output, dim=1)\n",
    "                predicted_mask = torch.argmax(probabilities, dim=1)\n",
    "                return predicted_mask.squeeze().cpu().numpy()  # shape [256, 256]\n",
    "            except Exception as e:\n",
    "                print(f\"Error in prediction: {e}\")\n",
    "                return np.zeros((256, 256), dtype=np.int64)\n",
    "    \n",
    "    def evaluate_sample(self, dataloader):\n",
    "        try:\n",
    "            images, masks = next(iter(dataloader))\n",
    "            image = images[0].to(self.device)\n",
    "            mask = masks[0].cpu().numpy()\n",
    "            pred_mask = self.predict(image)\n",
    "            \n",
    "            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            axs[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "            axs[0].set_title('Original Image')\n",
    "            axs[0].axis('off')\n",
    "            \n",
    "            colors = ['black', 'red', 'yellow', 'green']\n",
    "            cmap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "            axs[1].imshow(mask, cmap=cmap, vmin=0, vmax=3)\n",
    "            axs[1].set_title('Ground Truth')\n",
    "            axs[1].axis('off')\n",
    "            \n",
    "            axs[2].imshow(pred_mask, cmap=cmap, vmin=0, vmax=3)\n",
    "            axs[2].set_title('Prediction')\n",
    "            axs[2].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"sample_evaluation_{time.strftime('%Y%m%d_%H%M%S')}.png\")\n",
    "            print(\"Evaluation sample saved as image file\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in evaluation: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# 5) Main script\n",
    "# ------------------------------------------------------------------------------------\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting retinal segmentation training script using SwinNet architecture\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Update these paths for your setup\n",
    "    image_dir = 'C:/Second_Sem/490/train_images'\n",
    "    haemorrhages_mask_dir = 'C:/Second_Sem/490/APTOS 2019 Blindness Detection Segmented/Haemorrhages/train_images'\n",
    "    hard_exudates_mask_dir = 'C:/Second_Sem/490/APTOS 2019 Blindness Detection Segmented/Hard Exudates/train_images'\n",
    "    microaneurysm_mask_dir = 'C:/Second_Sem/490/APTOS 2019 Blindness Detection Segmented/Microaneurysm/train_images'\n",
    "    \n",
    "    for dir_path in [image_dir, haemorrhages_mask_dir, hard_exudates_mask_dir, microaneurysm_mask_dir]:\n",
    "        if not os.path.exists(dir_path):\n",
    "            print(f\"WARNING: Directory does not exist: {dir_path}\")\n",
    "    \n",
    "    print(\"Creating dataset...\")\n",
    "    dataset = RetinalMultiClassDataset(\n",
    "        image_dir=image_dir,\n",
    "        haemorrhages_mask_dir=haemorrhages_mask_dir,\n",
    "        hard_exudates_mask_dir=hard_exudates_mask_dir,\n",
    "        microaneurysm_mask_dir=microaneurysm_mask_dir\n",
    "    )\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"ERROR: Dataset is empty. Please check your directory paths and image files.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset created with {len(dataset)} samples\")\n",
    "    \n",
    "    print(\"Verifying dataset by loading first sample...\")\n",
    "    try:\n",
    "        sample_img, sample_mask = dataset[0]\n",
    "        print(f\"Sample image shape: {sample_img.shape}\")\n",
    "        print(f\"Sample mask shape: {sample_mask.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load sample from dataset: {e}\")\n",
    "    \n",
    "    print(\"Creating data loader...\")\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Increase if you want faster loading (and your system supports it)\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(\"Initializing segmentation model...\")\n",
    "    segmentation = RetinalSegmentation(n_classes=4)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    segmentation.train(train_loader, num_epochs=20)\n",
    "    \n",
    "    try:\n",
    "        torch.save(segmentation.model.state_dict(), 'retinal_segmentation_model_swin.pth')\n",
    "        print(\"Model saved successfully to 'retinal_segmentation_model_swin.pth'\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save model: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training completed\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Unhandled exception in main: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
