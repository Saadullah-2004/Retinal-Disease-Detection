{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 214\u001b[0m\n\u001b[1;32m    211\u001b[0m explainer \u001b[38;5;241m=\u001b[39m RetinalExplainer(model)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Generate and display explanation for a sample image\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_explanation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/devshah/Documents/WorkSpace/University/year 3/CSC490/Zero-Shot-Object-Tracking-FPS/APTOS 2019 Blindness Detection/train_images/0a4e1a29ffff.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[9], line 112\u001b[0m, in \u001b[0;36mRetinalExplainer.plot_explanation\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03mPlot the original image, heatmap, and superimposed visualization\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Generate explanation\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m superimposed_img, pred_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Load original image\u001b[39;00m\n\u001b[1;32m    115\u001b[0m original_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n",
      "Cell \u001b[0;32mIn[9], line 90\u001b[0m, in \u001b[0;36mRetinalExplainer.explain\u001b[0;34m(self, image_path, save_path)\u001b[0m\n\u001b[1;32m     87\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m preprocess(image)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Generate CAM\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m cam, pred_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_cam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_cam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Convert CAM to heatmap\u001b[39;00m\n\u001b[1;32m     93\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mapplyColorMap(np\u001b[38;5;241m.\u001b[39muint8(\u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m cam), cv2\u001b[38;5;241m.\u001b[39mCOLORMAP_JET)\n",
      "Cell \u001b[0;32mIn[9], line 61\u001b[0m, in \u001b[0;36mGradCAM.generate_cam\u001b[0;34m(self, input_image, target_class)\u001b[0m\n\u001b[1;32m     58\u001b[0m cam \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(activations, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [196]\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Reshape to square\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m width \u001b[38;5;241m=\u001b[39m height \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mmath\u001b[49m\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m196\u001b[39m))  \u001b[38;5;66;03m# 14x14\u001b[39;00m\n\u001b[1;32m     62\u001b[0m cam \u001b[38;5;241m=\u001b[39m cam\u001b[38;5;241m.\u001b[39mview(width, height)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# ReLU and normalize\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model.model\n",
    "        self.target_layer = self.model.vit.layernorm\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        \n",
    "        def save_gradient(module, grad_input, grad_output):\n",
    "            self.gradients.append(grad_output[0].detach())\n",
    "            \n",
    "        def save_activation(module, input, output):\n",
    "            self.activations.append(output.detach())\n",
    "            \n",
    "        self.target_layer.register_forward_hook(save_activation)\n",
    "        self.target_layer.register_full_backward_hook(save_gradient)\n",
    "\n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        \n",
    "        image_tensor = input_image.unsqueeze(0).to(self.model.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self.model(image_tensor).logits\n",
    "        pred_class = output.argmax(dim=1).item() if target_class is None else target_class\n",
    "        \n",
    "        # Clear gradients\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Backward pass\n",
    "        class_loss = output[0, pred_class]\n",
    "        class_loss.backward()\n",
    "        \n",
    "        # Get gradients and activations\n",
    "        gradients = self.gradients[0]  # [1, 197, 768]\n",
    "        activations = self.activations[0]  # [1, 197, 768]\n",
    "        \n",
    "        # Remove the class token (first token)\n",
    "        gradients = gradients[:, 1:, :]  # [1, 196, 768]\n",
    "        activations = activations[:, 1:, :]  # [1, 196, 768]\n",
    "        \n",
    "        # Pool the gradients across the patches\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 1])  # [768]\n",
    "        \n",
    "        # Weight the activations by the gradient importance scores\n",
    "        for i in range(768):\n",
    "            activations[:, :, i] *= pooled_gradients[i]\n",
    "            \n",
    "        # Average over the feature dimension\n",
    "        cam = torch.mean(activations, dim=2).squeeze(0)  # [196]\n",
    "        \n",
    "        # Reshape to square\n",
    "        width = height = int(math.sqrt(196))  # 14x14\n",
    "        cam = cam.view(width, height)\n",
    "        \n",
    "        # ReLU and normalize\n",
    "        cam = torch.maximum(cam, torch.tensor(0, device=self.model.device))\n",
    "        if cam.max() != 0:\n",
    "            cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "        \n",
    "        # Resize to match input image dimensions\n",
    "        cam = cam.cpu().numpy()\n",
    "        cam = cv2.resize(cam, (input_image.shape[2], input_image.shape[1]))\n",
    "        \n",
    "        return cam, pred_class\n",
    "\n",
    "class RetinalExplainer:\n",
    "    def __init__(self, classifier_model):\n",
    "        self.grad_cam = GradCAM(classifier_model)\n",
    "        self.model = classifier_model\n",
    "        \n",
    "    def explain(self, image_path, save_path=None):\n",
    "        \"\"\"\n",
    "        Generate and optionally save explanation visualization\n",
    "        \"\"\"\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        preprocess = self.model.transform\n",
    "        input_tensor = preprocess(image).to(self.model.model.device)\n",
    "        \n",
    "        # Generate CAM\n",
    "        cam, pred_class = self.grad_cam.generate_cam(input_tensor)\n",
    "        \n",
    "        # Convert CAM to heatmap\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "        \n",
    "        # Load original image and resize to match heatmap\n",
    "        original_image = cv2.imread(image_path)\n",
    "        original_image = cv2.resize(original_image, (cam.shape[1], cam.shape[0]))\n",
    "        \n",
    "        # Superimpose heatmap on original image\n",
    "        superimposed_img = cv2.addWeighted(original_image, 0.7, heatmap, 0.3, 0)\n",
    "        \n",
    "        if save_path:\n",
    "            cv2.imwrite(save_path, superimposed_img)\n",
    "            \n",
    "        return superimposed_img, pred_class\n",
    "\n",
    "    def plot_explanation(self, image_path):\n",
    "        \"\"\"\n",
    "        Plot the original image, heatmap, and superimposed visualization\n",
    "        \"\"\"\n",
    "        # Generate explanation\n",
    "        superimposed_img, pred_class = self.explain(image_path)\n",
    "        \n",
    "        # Load original image\n",
    "        original_image = cv2.imread(image_path)\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Plot original image\n",
    "        ax1.imshow(original_image)\n",
    "        ax1.set_title('Original Image')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Plot superimposed visualization\n",
    "        ax2.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
    "        ax2.set_title(f'Grad-CAM Visualization\\nPredicted Class: {pred_class}')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from torchvision import transforms\n",
    "    \n",
    "\n",
    "class RetinalClassifier:\n",
    "    def __init__(self, num_classes=5):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ViTForImageClassification.from_pretrained(\n",
    "            'google/vit-base-patch16-224',\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def train(self, train_loader, num_epochs=10):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs).logits\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            epoch_loss = running_loss/len(train_loader)\n",
    "            epoch_accuracy = 100 * correct / total\n",
    "            print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    def predict(self, image_path):\n",
    "        self.model.eval()\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image).logits\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "        return predicted.item()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming we have a trained classifier\n",
    "\n",
    "    model = RetinalClassifier()\n",
    "    \n",
    "    # Load the saved dictionary\n",
    "    checkpoint = torch.load('best_model.pth')\n",
    "    \n",
    "    # Load the state dictionary into the model\n",
    "    model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Create explainer with loaded model\n",
    "    explainer = RetinalExplainer(model)\n",
    "    \n",
    "    # Generate and display explanation for a sample image\n",
    "    fig = explainer.plot_explanation('/Users/devshah/Documents/WorkSpace/University/year 3/CSC490/Zero-Shot-Object-Tracking-FPS/APTOS 2019 Blindness Detection/train_images/0a4e1a29ffff.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
